[2023-04-23T18:25:20.662-0300] {taskinstance.py:1090} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: trabajo_practico.FiltrarDatos scheduled__2023-04-22T21:25:02.047239+00:00 [queued]>
[2023-04-23T18:25:20.665-0300] {taskinstance.py:1090} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: trabajo_practico.FiltrarDatos scheduled__2023-04-22T21:25:02.047239+00:00 [queued]>
[2023-04-23T18:25:20.665-0300] {taskinstance.py:1288} INFO - 
--------------------------------------------------------------------------------
[2023-04-23T18:25:20.665-0300] {taskinstance.py:1289} INFO - Starting attempt 1 of 2
[2023-04-23T18:25:20.665-0300] {taskinstance.py:1290} INFO - 
--------------------------------------------------------------------------------
[2023-04-23T18:25:20.680-0300] {taskinstance.py:1309} INFO - Executing <Task(PythonOperator): FiltrarDatos> on 2023-04-22 21:25:02.047239+00:00
[2023-04-23T18:25:20.683-0300] {standard_task_runner.py:55} INFO - Started process 29201 to run task
[2023-04-23T18:25:20.684-0300] {standard_task_runner.py:82} INFO - Running: ['airflow', 'tasks', 'run', 'trabajo_practico', 'FiltrarDatos', 'scheduled__2023-04-22T21:25:02.047239+00:00', '--job-id', '33', '--raw', '--subdir', 'DAGS_FOLDER/dag_tp.py', '--cfg-path', '/tmp/tmp241i_5dn']
[2023-04-23T18:25:20.685-0300] {standard_task_runner.py:83} INFO - Job 33: Subtask FiltrarDatos
[2023-04-23T18:25:20.718-0300] {task_command.py:389} INFO - Running <TaskInstance: trabajo_practico.FiltrarDatos scheduled__2023-04-22T21:25:02.047239+00:00 [running]> on host pepino-ThinkPad-E15-Gen-2
[2023-04-23T18:25:20.809-0300] {taskinstance.py:1516} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=trabajo_practico
AIRFLOW_CTX_TASK_ID=FiltrarDatos
AIRFLOW_CTX_EXECUTION_DATE=2023-04-22T21:25:02.047239+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-04-22T21:25:02.047239+00:00
[2023-04-23T18:25:20.877-0300] {python.py:177} INFO - Done. Returned value was:               advertiser_id product_id        type        date
35020  LW045DVYSGRD75TK6U54     ns6tzn  impression  2023-04-22
35021  L6WM4665XZVS9MCZRTVI     6jxm1s  impression  2023-04-22
35022  M0LU6DCI1WILGQBZ6808     z4xlup  impression  2023-04-22
35023  9Z77N44VDW6KX6VBWJ4X     n278ml  impression  2023-04-22
35024  8C88YB6E8YCGWU07HA7A     2a0ckb  impression  2023-04-22
...                     ...        ...         ...         ...
36679  KD9PHCBGYFBRI9ET1O9R     65hgvy  impression  2023-04-22
36680  IDOFCO721HTJGDH7332G     a5axui  impression  2023-04-22
36681  M0LU6DCI1WILGQBZ6808     12gijx  impression  2023-04-22
36682  AK81O7W3KGPEN8LABG2N     n04sl2       click  2023-04-22
36683  8C88YB6E8YCGWU07HA7A     2ehbaj  impression  2023-04-22

[1335 rows x 4 columns]
[2023-04-23T18:25:20.882-0300] {xcom.py:629} ERROR - Object of type DataFrame is not JSON serializable. If you are using pickle instead of JSON for XCom, then you need to enable pickle support for XCom in your airflow config or make sure to decorate your object with attr.
[2023-04-23T18:25:20.883-0300] {taskinstance.py:1776} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/pepino/.virtualenvs/apache_airflow/lib/python3.10/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/pepino/.virtualenvs/apache_airflow/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 2298, in xcom_push
    XCom.set(
  File "/home/pepino/.virtualenvs/apache_airflow/lib/python3.10/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/pepino/.virtualenvs/apache_airflow/lib/python3.10/site-packages/airflow/models/xcom.py", line 234, in set
    value = cls.serialize_value(
  File "/home/pepino/.virtualenvs/apache_airflow/lib/python3.10/site-packages/airflow/models/xcom.py", line 627, in serialize_value
    return json.dumps(value, cls=XComEncoder).encode("UTF-8")
  File "/usr/lib/python3.10/json/__init__.py", line 238, in dumps
    **kw).encode(obj)
  File "/home/pepino/.virtualenvs/apache_airflow/lib/python3.10/site-packages/airflow/utils/json.py", line 176, in encode
    return super().encode(o)
  File "/usr/lib/python3.10/json/encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "/usr/lib/python3.10/json/encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "/home/pepino/.virtualenvs/apache_airflow/lib/python3.10/site-packages/airflow/utils/json.py", line 170, in default
    return super().default(o)
  File "/usr/lib/python3.10/json/encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type DataFrame is not JSON serializable
[2023-04-23T18:25:20.885-0300] {taskinstance.py:1327} INFO - Marking task as UP_FOR_RETRY. dag_id=trabajo_practico, task_id=FiltrarDatos, execution_date=20230422T212502, start_date=20230423T212520, end_date=20230423T212520
[2023-04-23T18:25:20.898-0300] {standard_task_runner.py:100} ERROR - Failed to execute job 33 for task FiltrarDatos (Object of type DataFrame is not JSON serializable; 29201)
[2023-04-23T18:25:20.937-0300] {local_task_job.py:212} INFO - Task exited with return code 1
[2023-04-23T18:25:20.953-0300] {taskinstance.py:2596} INFO - 0 downstream tasks scheduled from follow-on schedule check
